{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(r'C:\\Users\\Anestis\\Desktop\\Limbo\\4_IEEE\\AI_SG_NLP_COURSE\\Preprocessing\\ai.txt', mode=\"r\", encoding=\"utf8\")\n",
    "content=f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Παράδειγμα\n",
    "Εύρεση όλων των αριθμών (χρονολογίες) στην παράγραφο του αρχείου ai.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1940', '1950', '1956']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "result=pattern.findall(content)\n",
    "print(result)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ΑΣΚΗΣΗ 1\n",
    "Φορτώστε το αρχείο ai.txt, το οποίο περιέχει μία παράγραφο για την ιστορία της Τεχνητής Νοημοσύνης από την Wikipedia. Μόνο με τη χρήση Regular Expressions βρείτε πόσες λέξεις ξεκινούν με κεφαλαίο γράμμα. \n",
    "HINT: Χρησιμοποιήστε τον κώδικα του παραδείγματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Κατά', 'Καθώς', 'Β', 'Παγκόσμιο', 'Πόλεμο', 'Το', 'Τούρινγκ', 'Τούρινγκ', 'Η', 'Αμερικανών', 'Τζον', 'Μακάρθι', 'Μάρβιν', 'Μίνσκυ', 'Κλοντ', 'Σάνον', 'Τη']\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "pattern = re.compile(r\"[Α-Ω]\\w*\")\n",
    "\n",
    "result=pattern.findall(content)\n",
    "print(result)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ΑΣΚΗΣΗ 2\n",
    "Φορτώστε το αρχείο ai.txt, το οποίο περιέχει μία παράγραφο για την ιστορία της Τεχνητής Νοημοσύνης από την Wikipedia. Μόνο με τη χρήση Regular Expressions τυπώστε μόνο τις λέξεις που περιέχουν \"Σ, σ ή ς\". \n",
    "HINT: Χρησιμοποιήστε τον κώδικα του παραδείγματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ΑΣΚΗΣΗ 3\n",
    "Φορτώστε το αρχείο ai.txt, το οποίο περιέχει μία παράγραφο για την ιστορία της Τεχνητής Νοημοσύνης από την Wikipedia. Μόνο με τη χρήση Regular Expressions τυπώστε μόνο τα περιεχόμενα των παρενθέσεων του κειμένου. \n",
    "HINT: Χρησιμοποιήστε τον κώδικα του παραδείγματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ΑΣΚΗΣΗ 4\n",
    "Φορτώστε το αρχείο ai.txt, το οποίο περιέχει μία παράγραφο για την ιστορία της Τεχνητής Νοημοσύνης από την Wikipedia. Μόνο με τη χρήση Regular Expressions τυπώστε μόνο τις λέξεις χωρίς τόνο. \n",
    "HINT: Χρησιμοποιήστε τον κώδικα του παραδείγματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ΑΣΚΗΣΗ 5\n",
    "Φορτώστε το αρχείο intelligence.txt. Μόνο με τη χρήση Regular Expressions να αντικαταστήσετε τα σύμβολα που δεν είναι γράμματα της αγγλικής αλφαβήτου με κενό χαρακτήρα \" \" (space).\n",
    "HINT: Χρησιμοποιήστε τον κώδικα του παραδείγματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, Lower Casing and Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Παράδειγμα \n",
    "Εφαρμογή tokenization και lower casing στην πρόταση \"This is a sample sentence to learn tokenization\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sample sentence to learn tokenization\n",
      "['this', 'is', 'a', 'sample', 'sentence', 'to', 'learn', 'tokenization']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This is a sample sentence to learn tokenization\"\n",
    "low_sent = ''\n",
    "for c in sentence:\n",
    "    low_sent += c.lower()\n",
    "\n",
    "tokens = low_sent.split()\n",
    "print(low_sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Παράδειγμα \n",
    "Αφαίρεση stopwords από την πρόταση \"This is a sample sentence to learn how to remove the stopwords.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'to', 'learn', 'how', 'to', 'remove', 'the', 'stopwords', '.']\n",
      "['This', 'sample', 'sentence', ',', 'learn', 'remove', 'stopwords', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"This is a sample sentence to learn how to remove the stopwords.\"\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "sent_token = word_tokenize(sentence)\n",
    "\n",
    "\n",
    "# Sentence to lower\n",
    "filtered_sentence = [w for w in sent_token if not w.lower() in stop_words] \n",
    "filtered_sentence = []\n",
    "\n",
    "# Remove stopwords \n",
    "for w in sent_token:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(sent_token)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ΑΣΚΗΣΗ 1 \n",
    "Φορτώστε το αρχείο intelligence.txt (by Wikipedia) ή χρησιμοποιήστε το αποτέλεσμα της τελευταίας άσκησης (ΑΣΚΗΣΗ 5) από τις Regular Expressions και προσπαθήστε να αφαιρέσετε τα stopwords όπως στο παράδειγμα. Αποθηκεύστε το αποτέλεσμα σε μία μεταβλητή και τυπώστε το."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Γνωστοί αλγόριθμοι Stemming (μτφρ. \"Στελεχοποίηση\") => απλοποίηση λέξεων (αφαίρεση πχ επιθεμάτων):\n",
    "- Porter's Stemmer\n",
    "- Lovins Stemmer\n",
    "- Paice Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programmer  :  programm\n",
      "programming  :  program\n",
      "programmers  :  programm\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
    " \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization (μτφρ. Ληματοποίηση):\n",
    "\"Σωστή μείωση στην μορφολογία των λέξεων\"\n",
    "π.χ. \"the boy's cars are different colors → the boy car be different color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universities : university\n",
      "playing : playing\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"universities :\", lemmatizer.lemmatize(\"universities\"))\n",
    "print(\"playing :\", lemmatizer.lemmatize(\"playing\"))\n",
    " \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11ba84cc61e1652d49e5ce6e5816725147cb920be3d6a0cdb7ae5e163bf55bcc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
